{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.misc import imresize, imread\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Introduction </center>\n",
    "\n",
    "In this tutorial we work on breast histology images. Our goal is to classify Invasive Ductal Carcinoma (IDC) images vs non IDC images using a standard data science pipeline : \n",
    "- Explore your dataset\n",
    "- Define your objective and a metric\n",
    "- Set a baseline algorithm : features / model / parameter optimization\n",
    "- Improve it\n",
    "    \n",
    "\n",
    "\n",
    "The dataset has been curated from [Andrew Janowczyk website](http://www.andrewjanowczyk.com/use-case-6-invasive-ductal-carcinoma-idc-segmentation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 1. Get your data </center>\n",
    "\n",
    "The first step is simply to get your data ! Data can have any modality : csv file, dicom image, text file, mp3 record... \n",
    "\n",
    "Then, using your favorite programming language (mainly python or R!) and appopriated packages (pandas, numpy...) you can read your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I did the work for you ! \n",
    "X = np.load('X.npy') # images\n",
    "Y = np.load('Y.npy') # labels associated to images (0 = no cancer, 1 = cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 2. Explore the dataset  </center>\n",
    "\n",
    "Let's explore your dataset now ! It is fundamental to become familiar with your data, especially if you're not an expert. This step includes :\n",
    "- Checking data issues of any sort : missing values, outliers, unreadable files etc.\n",
    "- Searching **bias in the dataset**. For instance imagine you have to predict the age of a patient based on is height, weight and other values... and you didn't see that birth of date was in these values : your algorithm is going to learn a completly dumb thing.\n",
    "- Plot histograms, correlations, use standard data analysis techniques sur as PCA\n",
    "- Try to evaluate the difficulty of the task you want to perform\n",
    "- And of course, have a look to related works on this dataset ! Maybe someone did the work for you :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many images ?\n",
    "\n",
    "print('Total number of images : {}'.format(len(X)))\n",
    "print('Number of images without cancer : {}'.format(np.sum(Y==0)))\n",
    "print('Number of images with cancer : {}'.format(np.sum(Y==1)))\n",
    "print('Percentage of positive images : {:.2f}%'.format(100*np.mean(Y)))\n",
    "\n",
    "# What is the size of the images ?\n",
    "\n",
    "print('Image shape (number of rows, number of columns, channels RGB): {}'.format(X[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Looking at some images\n",
    "\n",
    "imgs0 = X[Y==0] # images with label 0 = no cancer, negative image\n",
    "imgs1 = X[Y==1] # images with label 1 =    cancer, positive image\n",
    "\n",
    "batch = 0 # Each batch plot 50 negative images on the left and 50 positive images on the right\n",
    "\n",
    "for row in range(10):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for col in range(5):\n",
    "        plt.subplot(1,11,col+1)\n",
    "        plt.imshow(imgs0[50*batch + 5*row+col])\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1,11,col+7)\n",
    "        plt.imshow(imgs1[50*batch + 5*row+col])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe that colors are quite different between positive images and negative images. Colors could be an easy way to classify these images ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 3. Define your objective and metrics </center>\n",
    "\n",
    "Now that you are familiar with your data, you have to define your objective and the way you will measure how you achieved it. The machine learning scheme is quite different from the statistics scheme : we evaluate a model based on its predictive performance and not on the probability to observe the data given the model.\n",
    "\n",
    "In our case, the objective (or task) is **binary classification** of images and our measure performance will be the **accuracy** (percentage of good classification). Note that accuracy is not always a good measure : if you have an unbalanced dataset with only 1% of positive examples, predicting always 0 (negative) will lead to a 99% accuracy !\n",
    "\n",
    "The measure is often called **metric or loss**. It depends on what you want, interested by the trade off precision recall ? use F1 score. Interested by the trade-off true positive detection / false positive detection ? Use ROC-AUC. \n",
    "\n",
    "To measure the predictive performance using our metric, we will use **cross validation**. The idea is to split the dataset into 2 groups : the training set that we will use to calibrate the parameters of a model, and the test set that we will use to evaluate the performance of the model using our metric. \n",
    "\n",
    "In cross validation, we split the dataset multiple times (ex 80/20 or 90/10), and average the performances on the diffrent test sets. A common technique is to partition the dataset into N equal parts, called **folds**,  and to split N times the dataset such that each part will be once the test set. For instance if you divide your dataset into 5 folds [1,2,3,4,5] then you will split 5 times the dataset : \n",
    "\n",
    "- train = [2,3,4,5] test = [1] > evaluate performance on the test set to get score1\n",
    "- train = [1,3,4,5] test = [2] > ... score2 \n",
    "- train = [1,2,4,5] test = [3] > ... score3 \n",
    "- train = [1,2,3,5] test = [4] > ... score4 \n",
    "- train = [1,2,3,4] test = [5] > ... score5 \n",
    "- predictive performance = (score1 + score2 + score3 +score4 +score5)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold as KFold\n",
    "\n",
    "# Prepare the folds for a cross validation. We use the very useful scikit learn package. Here we use stratified \n",
    "# cross validation : the proportion of positive and negative examples is the same in all folds\n",
    "\n",
    "N = len(X)\n",
    "folds = lambda : KFold(n_splits = 5, shuffle = True, random_state=0).split(X, Y)\n",
    "\n",
    "# Let's have a look to the split size and \n",
    "\n",
    "for i, (train_indexes, test_indexes) in enumerate(folds()):\n",
    "    print('split {}: training : {} images, test : {} images'.format(i + 1, len(train_indexes), len(test_indexes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 4. Set a baseline model  </center>\n",
    "\n",
    "Now it's time to create our first model ! A good practice is to set a **baseline**, i.e. a very simple model. It permits to quickly have a first score to compare with when using more complex models and it also often helps a lot to organize your code and data.\n",
    "\n",
    "A common machine learning way to create a new model is :\n",
    "1. Extract features from your data. In our case we saw that colors of the images could be a good feature. Here expert knowledge is welcome ! \n",
    "2. Use a learning algorithm you think appropriate : k-nearest neighbors, linear or logistic regression, random forests, SVMs, neural networks and **XGboost** \n",
    "3. Optimize the hyper-parameters of your learning algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 4.1. Create features </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We look at RGB histograms (represent colors). Histogram counts the number of pixels with a certain intensity\n",
    "# between 0 and 255 for each color red, green and blue. A peak at 255 for all colors mean a lot of white ! \n",
    "\n",
    "i= 1 # Try 0, 1, 2.. for negative images and -1, -2, -3 for positive images and compare the histograms.\n",
    "xi = X[i]\n",
    "\n",
    "plt.imshow(xi)\n",
    "plt.axis('off')\n",
    "plt.title('IDC' if Y[i] else 'not IDC')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "n_bins = 50\n",
    "plt.hist(xi[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n",
    "plt.hist(xi[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n",
    "plt.hist(xi[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our feature will be the concatenation of the 3  histograms: red, green and blue. \n",
    "\n",
    "histogram_features = []\n",
    "n_bins = 50 # We decide to keep 50 bins in the histogram, but you can try other values ! (up to 255)\n",
    "for xi in X:\n",
    "    hr = np.histogram(xi[:,:,0].flatten(), bins= n_bins)[0]\n",
    "    hg = np.histogram(xi[:,:,1].flatten(), bins= n_bins)[0]\n",
    "    hb = np.histogram(xi[:,:,2].flatten(), bins= n_bins)[0]\n",
    "    hi = np.concatenate([hr, hg, hb])\n",
    "    histogram_features.append(hi)\n",
    "\n",
    "histogram_features = np.array(histogram_features)\n",
    "\n",
    "print 'histogram shape : ', histogram_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we visualize our features in 2D using PCA, colored by the positive (red) / negative (blue) class\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, SpectralEmbedding\n",
    "\n",
    "model = PCA(n_components=2) # Try to replace PCA by SpectralEmbedding, TSNE !\n",
    "PC = model.fit_transform(histogram_features)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(PC[:,0], PC[:, 1], s= 10, c=['r' if yi else 'b' for yi in Y], alpha = 0.5)\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA shows that our simple histogram feature already permits to linearly separate a bit positive and negative images ! Good point ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 4.2. Run a simple algorithm </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we will use the k nearest neighbors algorithm : given a new image, find the k (e.g. k=10) images in the training set\n",
    "# with the most similar histograms. If m of these neighors (e.g m=6) have label 1, then output p = 60%.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "# This function run train the model ! \n",
    "def run_model(model, features):\n",
    "    Y_pred = np.zeros(Y.shape)\n",
    "    # For each fold :\n",
    "    for train_indexes, valid_indexes in folds():\n",
    "        # Train the model on the training data\n",
    "        model.fit(features[train_indexes], Y[train_indexes])\n",
    "        # And predict the results on the test data\n",
    "        Y_pred[valid_indexes] = model.predict_proba(features[valid_indexes])[:,1]\n",
    "    # Return all the predictions (probabilities between 0 and 1)\n",
    "    return Y_pred\n",
    "\n",
    "# Here we use kNN model : \n",
    "model = KNeighborsClassifier(n_neighbors=5, n_jobs=8)\n",
    "# And call the function run_model\n",
    "Y_pred = run_model(model, histogram_features)\n",
    "\n",
    "# Let's print the results\n",
    "print('10 first predictions in probability : {}'.format(Y_pred[:10]))\n",
    "Y_pred_01 = (Y_pred > 0.5).astype('int')\n",
    "print('10 first predictions 0/1 :  {}'.format(Y_pred_01[:10]))\n",
    "print('10 first real labels     :  {}'.format(Y[:10]))\n",
    "print('Accuracy : {:.2f}%'.format(100 * np.mean(Y == Y_pred_01)))\n",
    "\n",
    "print('Confusion matrix (see wikipedia for more info)')\n",
    "print(confusion_matrix(Y, Y_pred_01))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "fpr, tpr, _ = roc_curve(Y, Y_pred)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('AUC = {:.2f}'.format(roc_auc_score(Y, Y_pred)))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.plot([0,1],[0,1], 'black')\n",
    "print('AUC is the probability for a positive image to have a higher score than a negative image')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 4.3. Optimize the hyper-parameters of your algorithm  </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How does the accuracy evolves with the number of neighbors ?\n",
    "\n",
    "def plot_accuracy(model, features, x_range):\n",
    "\n",
    "    accuracy = []\n",
    "    for x in x_range:\n",
    "        accuracy.append(np.mean(Y == (run_model(model(x), features) > 0.5).astype('int')))\n",
    "        print('accuracy = {:.2f}% at x = {}'.format(100*accuracy[-1], x))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(x_range, accuracy)\n",
    "    plt.xlabel('Number of neighbors')\n",
    "    plt.xticks(x_range)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Max = {:.2f}% at x = {}'.format(100*np.max(accuracy), x_range[np.argmax(accuracy)]))\n",
    "    plt.grid()\n",
    "    \n",
    "plot_accuracy(lambda k : KNeighborsClassifier(n_neighbors=k, n_jobs=8), histogram_features, range(5,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 5. Improve features and model   </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 5.1. Better features  </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we use features extracted from a deep neural network : we achieve much better results with the same\"\n",
    "# algorithm that our simple histogram features ! \n",
    "\n",
    "resnet_features = np.load('resnet_features.npy')\n",
    "print resnet_features.shape\n",
    "\n",
    "# Nearest Neighbors\n",
    "plot_accuracy(lambda k : KNeighborsClassifier(n_neighbors=k, n_jobs=8), resnet_features, range(5,45,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code to extract deep learning features : a bit long to run ! \n",
    "\n",
    "#from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "#model = ResNet50(weights='imagenet', include_top=False, pooling='max)\n",
    "#X_224 = np.array([imresize(xi, (224, 224)) for xi in X])\n",
    "#X_224 = preprocess_input(X_224.astype('float'))\n",
    "#resnet_features = model.predict(X_224, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 5.2. Better Algorithms  </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logisitic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "Y_pred_01 = (run_model(LogisticRegression(n_jobs=8), histogram_features) > 0.5).astype('int')\n",
    "print 'Accuracy : {:.2f}%'.format(100* np.mean(Y == Y_pred_01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "plot_accuracy(lambda d : RandomForestClassifier(n_estimators = 300, max_depth = d, n_jobs=8),\n",
    "              histogram_features, range(5, 55, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 6. Use Deep Learning   </center>\n",
    "\n",
    "Deep learning for images is end to end : there is no need to exctract features from the image because the algorithm learns itself these features from raw data ! Neural networks are not easy to train, especially from scratch, and require a lot of data. But when possible, it shows amazing performance ! In the code below, we run quite a randomly designed neural network to see how it works. Don't expect it to have very good results ;) Also, use it as a black box as this tuto doesn't aim to explain how deep learning works ! Training a neural network can be long, so we just use 1 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras # We use keras library wuth tensorflow backend\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "# Special callback to see learning curves\n",
    "class MetricsCheckpoint(Callback):\n",
    "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
    "    def __init__(self, savepath):\n",
    "        super(MetricsCheckpoint, self).__init__()\n",
    "        self.savepath = savepath\n",
    "        self.history = {}\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        np.save(self.savepath, self.history)\n",
    "\n",
    "# Preprocess the data : center and normalize\n",
    "X2 = X / 255.\n",
    "X2 = X2 - np.mean(X2, axis = (0,1,2))\n",
    "X2 = X2 / np.std(X2, axis = (0,1,2))\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(50, 50, 3))) # first layer : convolution\n",
    "model.add(MaxPooling2D(pool_size=(3, 3))) # second layer : pooling (reduce the size of the image per 3) \n",
    "model.add(Conv2D(32, (5, 5), activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid')) # output 1 value between 0 and 1 : probability to have cancer\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy, # Use binary crossentropy as a loss function  \n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X2[train_indexes], Y[train_indexes],\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data = [X2[test_indexes], Y[test_indexes]],\n",
    "        callbacks = [MetricsCheckpoint('logs')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "metrics = np.load('logs.npy')[()]\n",
    "filt = ['acc'] # try to add 'loss' to see the loss learning curve\n",
    "for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
    "    l = np.array(metrics[k])\n",
    "    plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
    "    x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
    "    y = l[x]\n",
    "    plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
    "    plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')\n",
    "    \n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.axis([0, None, None, None]);\n",
    "plt.grid()\n",
    "plt.xlabel('Number of epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is trained by showing him the dataset multiple times. Each time the network saw the entire dataset is called epoch. On the learning curve you can observe the accuracy of the network on the training set in red and on the test set in blue. The more epochs, the more the networks learn to recognize the training imagss and generalize it's performance on the test set. But if we go too far, the network memorized perfectly the training set (very high accuracy on training) which leads to poor generalization (low accuracy on the test), this phenomenom is called overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
